#+STARTUP: latexpreview
* Programming massively parralel hardware

* 01/09 Intro lecture
  SCHEDULED: <2020-09-01 Tue>
** Course organization
The couse has a Software track and a Hardware track
Intended learning outcomes of this lecture
- List Homorphisms
- Moores law and hardware trends
- Explain what a list-homomorphic program is /and/
- Be able to apply it to build programs
- Illustrate and apply the 1st Theorem of List HomoMorphism to transform said programs into
  inherently parralel ones

*** Lectures
- Tue 10:15 - 12:00, on zoom
- Thu 10:15 - 12:00, on zoom

*** Labs
- Thu: 13-17, 3-0-25, DIKU
- Half a class physically
- Other half on zoom
- /Will/ Accept students from virtual half as long as we do not overtake corona(20)
- If possible, may stay longer
*PURPOSE*
- Not educative(but you can ask questions)
- Use weekly ass to learn practical stuff
- Can't complete them without physical help
- *very important to attend the labs*

*** Assignments
Anders will provide feedback
- Four individual weekly assignments: 40% of grade
- One group project + final presentation and discussion: 60%
  - May choose from possible projects
  - Presented during lab at some point
  - Or discuss our own project
  - Can be very practical in CUDA, Futhark or more theoretical


** Brief history
*** Trend towards ever-increasing hardware parralellism(HP)
- Moores law(1960s)
Yeah u got this one
Generally aknowledged to be slowed down at this point

- ICPP, ISCA (1980/90s): parallel architechtures 
International conference for parallel processors

- Mid90 Killer-micro
Parallel was mostly seen as "Exotic"
The path of least resistance was to increase the speed of a Single CPU
On a single cpu the programmers point of view didn't change. Therefore
there was a lower barrier of entry, and easier maintenance of legacy software
Increasing single process performance "benefits everyone" with no change
in source
Commercially multiprocessors was just an uniprocessor extension

- Problems
Problem was. Power usage is cubed of the frequency, approx $P_dynamic \approx freq^3$
The Memory Wall. Performance gap between proc and mem increases

In 2004 Intel shifted focus to multi-cores instead

*** Biggest challenge for PH
A duality between the hardware system, and the application.

- Applications have performance requirements. 
- Hardware systems grant Performance opportunities
  
Thus application pressures the hardware to perform, and hardware
pressures the application to exploit opportunities.

The example is Microsoft and Intel, which Cosmin thinks about all the time
/at night/

The biggest challenge is to develop efficient Massively-Parralel Software
Think about programs with parallelism in mind rather than hack /some/ 
parallelism out of a sequential implementation(se node)

We need to think about developing for an infinite amount of course, since
we don't know how many cores will be available

** PMPH Main topics
*Hardware track* is the physical components of hardware design
- Processor (ILP, Intra and Inter-core)
- Memory hierarchy (coherency)
  - Several copies in the cache of the same mem-block
- Interconnect (inter-cores or core-cache routing)

*The Software track* studies programming models for data parallelism and
a way to reason and optimize parallelism
- High-level of abstraction
  - List-homomorphism $\equiv$ functional map-reduce style and flattening
- Low-level:
  - Loops and transformations rooted in data dependency analysis
  - c-programming

    
*The Lab* track applies in practice the optimizations/transformations learned
in the software track

The lecture notes will explain things to some extent, hardware book isn't mandatory

The hardware track is /important/ BUT the focus will be on software

** Abstractions in this course
A /program/ is to a /process or thread/ what a recipe is for cooking
/A processor(core)/ is a hardware entity capable of sequencing &
executing insts
/MT Cores/ Multiple threads in own context

** Software(List homomorphism)
Consider the realm of finite lists
.. denites list concatenation
Empty list is the neutral element eg [] (..) x === x (..) [] === x

LH is a special form of divide and conquer
we denote
h [] = e
h [x] = f x
h (x .. y) = (h x) (merged with a binop) (h y)

A well-defined program requires that no matter
how the input list is partitioned into, x .. y 
the result is the same

(compute the length of the list)
#+BEGIN_SRC 
len :: [T] -> Int
len [] = 0
len [x] = one(x) //where one(n) = 1 aka const
len (x..y) = (len x) + (len y)
#+END_SRC
That is the basic idea

What about a function that computes whether a list satisfies predicat p

#+BEGIN_SRC 
all_p :: [T] -> Bool
all_p [] = True // Because later we will probably use /and/ to compare
all_p [x] = p (x)
all_p (x..y) = (all_p x) && (all_p y)
#+END_SRC
(all_p x) is whether all elements of p returns true

*** Definition of monoid
Assume set $S$ and $binop : S x S \rightarrow S$. $(S,binop)$ is called a Monoid
If it satisfies the two axioms
1. Associativity $\forall x,y,z \in S$ we have $(x\ binop\ y)\ binop\ z \equiv x\ binop\ (y\ op\ z)$
2. Identity element: There exists $e \in S$ st. $\forall a \in S, e\ op\ a\ \equiv a\ op\ r\ \equiv a$

*** Definition of Monoid Homomorphism
A monoid homomorphism from monoid (S,op) to monoid (T,op2) is a function
h: S -> T st. $\forall u,v\in S, h(u op v) \equiv h(u) op2 h(v)


** TODO Basic blocks: Map
Map :: (al -> be) -> [al] -> [be] has inherently parralel semantics
| x = map f | [ a1, a2, a3 ]      |
| x'        | [f a1, f a2, f a3 ] |

** TODO Basic blocks Reduce
reduce applies an operator across all list elements

** 1.st lh theorem[Meertens]
#+BEGIN_SRC
h [] = e
h [x] = f x
h (x .. y) = (h x) (merged with a binop) (h y)
h === (reduce binop e) applied to (map f)
#+END_SRC

So in previous example
#+BEGIN_SRC 
len :: [T] -> Int
len [] = 0
len [x] = one(x) //where one(n) = 1 aka const
len (x..y) = (len x) + (len y)

len === (reduce (+) 0) on (map one)
#+END_SRC

And all_p
#+BEGIN_SRC 
all_p :: [T] -> Bool
all_p [] = True // Because later we will probably use /and/ to compare
all_p [x] = p (x)
all_p (x..y) = (all_p x) && (all_p y)

(reduce (&) True) on (map p)
#+END_SRC

I caml
#+BEGIN_SRC ocaml
let tmp = map p arr
let result = reduce (&) True tmp
#+END_SRC


** Theorem List-Homomorphism Promotions
Given unary functions f,g and an associative binop then
1. Map f on map g == map(f o g)
2. Mapping a (map f) on a list of lists, is equiv [map f [a1,...ai], map f [aj,...an]
   If we're reducing this with concatenation we get [f a1, f a2, ..., f an]
3. (reduce /binop/ e_binop) . (reduce (..) []) is equiv to
   (reduce /binop/ e_binop) . (map (reduce /binop/ e_binop)

** Distr
Assume =distr_p= distributes a list into /p/ sublist of roughly the same number of elements
Generally /p/ is the amount of processors

This is useful for code gen
(map f) . (reduce (..) [] = (reduce (..) []) . (map (map f)) 
map f = (reduce (..) []) . (map (map f)) . distr_p

In this case we're distributing the original list into a list of sublists, eg:  
[1,2,3,4] -> [[1,2], [3,4] ]
The reduce will be sequential, but the initial maps will be parralelised

** Theorem: Optimized map reduce
Assume =distrp= :: [a] ->  [[al] ] distributes a list. Denoting
=redomap binop f e_binop is.equiv (reduce binop e) . (map f)= The equality holds:  
redomap binop f e is.equiv (reduce /binop/ e) . (map (redomap /binop/ f e)) . distr_p
(nb: not redo map, but reduce o map)
Essentially, since the maps are run in parralel, we can reduce the maps first, then
when we finally flatten, we reduce only the already reduced list


** Maximum Segment Sum Problem
A non-homeomorphjic function g can sometimes be lifted to a homomorphic one /f/
by computing a baggage of extra info

The initial function can be obtained by projecting the homomorphic result: 
$g = \pi . f$

MSSP:  
Given a list of integers, find the contiguous segment of the list whose members have 
the largest sum among all such segments. The result is only the sum

The naive attempt states:(where T denotes max)
#+BEGIN_SRC
mss [] = 0
mss [a] = a T 0
mss (x .. y) = mss(x) T? mss(y)
#+END_SRC
We get the issue that there may exist a larger mss that spans between the boundary between x and y

Therefore we compute extra information this time:
- Maximum concluding segment
  Always starts at the end, and whose sum is the maximum across such segments
- Maximum initial segment
  Always starts at the start, and whose sum is the max of such segments(inv. mcs)
- Total segment sum
  
At this point i introduce an example:
[1, -2, 3, 4, || -1, 5, -6, 1]
first list: 
- MSS = 7(3,4)
- MCS = 7(3,4)
- MIS = 6(1..4)
- TS  = 6
Second:
- MSS = 5
- MCS = 1(1)
- MIS = 4(-1,5)
- TS = -1

We now want to denote the functions =mis=,  
- /mis/ = max(mis_1, ts_1 + mis_2)  
  If the first initial is lower than the whole first segment + the second initial, then it spans 
  the whole first sublist and the initial of the second  
- /mcs/ = max(mcs_2, ts_2 + mcs_1)
  Same as before, but the other way aorund
- /mss/ = max(mss_1, mss_2, mcs_1 + mis_2)
  Either the mss of either split OR the split that crosses the boundary

#+BEGIN_SRC haskell
-- x T y = if(x >= y) then x else y
(mssx, misx, mcsx, tsx) O (mssy, misy, mcsy, tsy) = (
  mssx T mssy T (mcsx + misy),
  misx T (tsx+misy),
  (mcsx + tsy) T mcsy,
  tsx + tsy
)
f x = (x T 0, x T 0, x T 0)

emms = (reduce O (0,0,0,0)) . (map f)

mss = pi_1 . emss
# Where pi_1 (a,_,_,_) = a
#+END_SRC
The baggage is 3 extra integers (misx, mcsxm, tsx) and a constant number
of intops per communication stage
** Longest Satisfying Segment Problems
- A class of problems which requires to find the longest segment of a list
  for which some property holds such as
the longest sequence of zeros  
the longest sorted sequence

We restrict the shape of the predicate to:
#+BEGIN_SRC
p [] = True
P [x] = ...
p [x,y] = ...
p [ x : y : sz] = (p [x,y]) and p (y : zs)
#+END_SRC
We get extra baggage:
- As before, length of the longest initial/concluding satisfying segments (lis/lcs), and the 
  total list length (tl) 
- When considering the concat of (lcs,lis) pair, it is not guaranteed that the result 
  satisfies the predicate. E.G (sorted x) And (sorted y) != sorted x..y  
[1,2,3|2,3,4] both are sorted, but the concatenation isn't

therefore we need the last element of lcs and the first elem of lis

I.e. p [lastx,firsty] er sand så er de forbundet
sorted [3,2] == false så den forbinder ikke


* [lab] 03/09 Cuda intro
  <2020-09-03 Thu>
Don't initially install on personal computer(Du har 1080 derhjemme)  
Først ssh til proxy:
=ssh ssh-diku-apl.science.ku.dk -l wbr220=
Så til gpu maskinen
=ssh gpu02-diku-apl=
** GPGPU Programmering
Remember that CPU and GPU memory are not mutually accessible.
Copy data from cpu to gpu, compute, then copy from gpu to cpu
#+BEGIN_SRC c
cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice);
cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);
#+END_SRC
Copies x,y to d_x,d_y from host to device

Simple square function (map (\x -> x*x)
#+BEGIN_SRC c
  #include <stdlib.h>
  #include <stdio.h>
  #include <string.h>
  #include <math.h>
  #include <cuda_runtime.h>

  __global__ void squareKernel(float *d_in, float *d_out){
    const unsigned int lid = threadIdx.x; // Local id inside a block
    const unsigned int gid = blockIdx.x*blockDim.x + lid; // global id
    d_out[gid] = d_in[gid]*d_in[gid]; // Place result(square) in d_out
  }

  int main( int argc, char** argv){
    unsigned int N = 32;
    unsigned int mem_size = N*sizeof(float);
    // Init host mem
    float* h_in = (float*) malloc(mem_size);
    float* h_out = (float*) malloc(mem_size);
    for (unsigned int i=0; i<N; ++i) h_in[i] = (float)i;
  
    // Init device mem
    float* d_in;
    float* d_out;
    cudaMalloc((void**)&d_in, mem_size);
    cudaMalloc((void**)&d_out, mem_size);

    // Copy host mem to device
    cudaMemcpy(d_in, h_in, mem_size, cudaMemcpyHostToDevice);

    // Exec kernel
    squareKernel<<< 1, N>>>(d_in, d_out);

    // Copy result from device to host
    cudaMemcpy(h_out, d_out, mem_size, cudaMemcpyDeviceToHost);

    // Print result
    for(unsigned int i=0; i<N; ++i) printf("%.6f\n", h_out[i]);

    free(h_in); free(h_out);
    cudaFree(d_in); cudaFree(d_out);

    return 0;
  }

#+END_SRC

** Trouble ahead
When we write an assignment we run into issues when we write programs with larger arrays,
and therefore larger block sizes.

CUDA does not accept a block of size 32757
- A /CUDA warp/ is formed by 32 threads that execute SIMD
- A /CUDA Block/ may contain up to 1024 threads
- Synchro/communication is possible inside a cuda block by means of barriers
  and scratchpad(shared) memory
- Barrier synchro is *not* possible across threads in different CUDA blocks

If the size of the computation doesn't match exactly a multiple of block size, you need to spawn extra threads. 
Hence we need to ad an if inside kernel code, to make the extra threads idle

Look at the previous squarekernel:
#+BEGIN_SRC c
__global__ void squareKernel(float *d_in, float *d_out){
    const unsigned int lid = threadIdx.x; // Local id inside a block
    const unsigned int git = blockIdx.x*blockDim.x + lid; // global id
    d_out[gid] = d_in[gid]*d_in[gid]; // Place result(square) in d_out
}
#+END_SRC
There is an issue. We need to make sure gid isn't overshooting.

We now expand the kernel to take another param
#+BEGIN_SRC c
__global__ void squareKernel(float *d_in, float *d_out, int N){
    const unsigned int lid = threadIdx.x; // Local id inside a block
    const unsigned int git = blockIdx.x*blockDim.x + lid; // global id
    if(gid < N){
	d_out[gid] = d_in[gid]*d_in[gid]; // Place result(square) in d_out
    }
}
// And when executing:
unsigned int N = 32757;
unsigned int block_size = 256;
unsigned int num_blocks = ((N+(block_size-1) / block_size);
squarekernel<<< num_blocks, block_size>>>(d_in,d_out,N)
#+END_SRC
And this will be the basis for the first assignment
* 03/09-08/09  Flattening
  SCHEDULED: <2020-09-03 Thu>

** Amdahl's law
Enhancement accelerates a fraction F of the task by a factos S
1. Speedup is limited by 1 - F that cannot be optimised
2. Optimize the common case & execute the rare case in software
3. Law of diminishing returns

In the context of Parallel Speedup
If we leave something sequential, it will become a bottleneck
as the amount of processors increases.

Typically speedup is sublinear, e.e. due to inter-thread communication

Even if 99% of the code is parralelized. We cannot move faster than the
1%

** PRAM Parallel Access Machine
Focuses exclusively on parallelism and ignores issues related to
synchro and comm:
1. p processors connected to shared memory
2. each processor has an unique id /i/
3. SIMD Execution, each parallel instruction requires unit time
4. Each processor has a flag that controls whether it is active in the execution of an instruction

The work time algorith WT
- Work Complexity W(n) is the total number of ops performed
- Depth/step complexity D(n) is the total number of sequential steps
If we know WT's work and depth, then Brents Theorem gives a good complexity bound
for a PRAM

** Brent Theorem
A work-Time algorithm of Depth D(n) and work W(n) can be simulated on a P-processor PRAM in time complex T  
$$ \frac{W(n)}{P} \leq T < \frac{W(n)}{P} + D(n) $$

** Futhark functions
*** Map2
#+BEGIN_SRC
map2: (a -> a -> b) -> [a] -> [a] -> b
map2: O [a_1,...,a_n] [b_1,...,b_n] = [a_1 O b_1, ..., a_n O b_n]
#+END_SRC

*** Filter
#+BEGIN_SRC
pred = odd x = (x % 2) == 1
filter odd [3,6,8,7,10]
> [3,7]
#+END_SRC

*** Scatter
A parallel write operater
Scarer updates in parallel a base array with a set of values

/NB: To support in-place updates, we use * to define a "mutable" function/
#+BEGIN_SRC
scatter: *[m]a -> [n]int -> [n]a -> *[m]a

X (input array)  = [a0, a1, a2, a3, a4, a5]
I (index vector) = [ 2, 4, 1,-1]
A (data vector)  = [b0,b1,b2,b3]
scatter X I A    = [a0, b2, b0, a3, b1, a5]
#+END_SRC
When index is out of bounds, the input is ignored

Scatter can be used to update out of order. If Index 
vector is iota, then it is equivalent to a map.

Scatter has D(n) = O(1) and W(n) = O(n)  
Array X is consumed by scatter, and can't be used afterwards

*** Permute, split, replicate, iota
Operator to permute in parallel based on a set of indixes
TODO

*** Partition2/Filter
#+BEGIN_SRC
partition2 : (a->Bool) -> [n]a -> (i32, [n]a)
#+END_SRC
In result, the elements satisfying the predicate occur before the others 
Can be implemented by means of map, scan and scatter

Vi tænker at starte med filter
#+BEGIN_SRC
odd [1,4,6,7,10,11]
> [1,7,11,4,6,10]
EG Index rykker sig til
[0->0,1->3,2->4,3->1,4->5,5->2]

Første trin er 
map pred arr: [1,0,0,1,0,1](alle der matcher pred)
Andet er
exclusive_scan + 
: [0,1,1,1,2,2] (now 0 1 and 2 are in the right pos)
scatter
#+END_SRC
Vi kører de første to trin igen, nu med inverted predicate,
og adder offset(antal sande elementer) til resultatet  

Vi zipper de to index arrays, og mapper over functionen:
#+BEGIN_SRC
let inds = map(\(c,iT,iF) ->
    if c then iT-1 else iF-1
) (zip3 cs isT isF)
#+END_SRC
Hvor cs = =map cond X=  
Hvor isT= index arr hvis val er True  
Hvor isF= index arr hvis val er False  

Så bruger vi =scatter (replicate n 0) inds og_arr= som
indsætter according til den liste af indices

** Computing prime numbers up to N
Start with an array of size n filled initially with 1, i e all are primes.
Iteratively zero out all multiples of numbers up to sqrtjn)  
**Eratosthenes Sieve**
#+BEGIN_SRC
int res[n] o {0,0,1,1,1,...,1}
for(i o 2; i <= sqrt(n); i++){
  if ( res[i] != 0) {
    forall m in multiples of i <= n do{
      res[m] = 0; // kan implementeres som scatter
    }
  }
}
#+END_SRC
Work: O(n lg lg n) but Depth: O(sqrt(n))

*** Bedre løsning
Hvis vi har alle primer fra 2 til sqrt(n) så kan vi generere alle multiples
af disse primes på en gang ={2*p:n:p]: p in sqr_primes}=.

Vi kan så kalde recursivt på n -> sqrt(n) -> sqrt(sqrt(n))
 indtil vi rammer et base-case.

** Flattening
Why flattening?  
- GPU Hardware doesn't support recursion efficiently.  
- We split onto a grid of blocks
- A way to map arbitrary functions to flat hardware


Simple and incomplete recipe
1. Normalize the program
2. Start distributing outer-maps across its containing let-
3. Whenever the body of the map is exactly a map or a reduce
   or a scan, iota, replicate etc. Apply the corresponding re-write rule

*** Simple example

#+BEGIN_SRC
let arr = [1, 2, 3, 4] in
map (\i -> map (+(i+1)) (iota i)) arr
-- Result: [[2],[3,4],[4,5,6],[5,6,7,8]]
#+END_SRC
1. Normalise the code
#+BEGIN_SRC
let arr = [1, 2, 3, 4] in
map (\i ->
    let ip1 = i+1 in
    let iot = iota i in
    let ip1r= (replicate i ip1) in
    map2 (+) ip1r iot ... ) arr

-- Result: [[2],[3,4],[4,5,6],[5,6,7,8]]
#+END_SRC
2. Distribute the map over every inst
$F$ Denote the flattening tranf and modifies the input
#+BEGIN_SRC
F(map ...) ===
1. let ip1s = map(\i -> i+1) arr in
2. let iots = F(map(\i -> (iota i)) arr)
3. let ip1rs= F(map2 (\i ip1 -> (replicate i ip1)) arr ip1s)
4. in F(map2 (\ip1r iot -> map2 (+) ip1r iot)ip1rs iots)
#+END_SRC
i 1 er den allerede map over sekventielt  
i 2 kan vi omskrive (iota i) og gøre den flad




#+BEGIN_SRC
#+END_SRC
* [lab 10/09] Cuda scan and reduce
In Cuda, we split the job into block. Each block gets implemented into a
"streaming multiprocessor" SM with various stuff like shared memory

- A block contains many threads
- A block /can't/ run on more than one SM
- Threads in a block can cooperate
- Threads on the same SM /can't/ cooperate
  The hardware allows this, however the software doesn't allow these to cooperate. We
have no way to guarantee which blocks run on which SMs
- The programmer /can't/ specify the order two blocks of the same kernel will run
- The programmer can specify the order two block with different kernels will run

** Synchronization
Collaborate in block using shared mem
#+BEGIN_SRC c
__global__ void kernelCall(){
    extern __shared__ int sharedArr
}
...
kernelCall<<<blocks, threads, shared_mem_size>>>(...)
#+END_SRC

I assume we want to shift every element of an array left by one
#+BEGIN_SRC c
__global__ void shiftLeftkernel(float *d_in, float *d_out){
  extern __shared__ float s_arr[];
  s_arr[tid] = d_in[tid];
  s_arr[tid] = s_arr[tid+1];
  d_out[tid] = s_arr[tid];

}
#+END_SRC
With an "obvious" bug

#+BEGIN_SRC c
__global__ void shiftLeftkernel(float *d_in, float *d_out){
  extern __shared__ float s_arr[];
  // S_arr[tid] might be already written by another thread
  s_arr[tid] = d_in[tid];    // These 
  // We might overwrite something
  s_arr[tid] = s_arr[tid+1]; // Two lines
  // s_arr[tid] might not be set

  d_out[tid] = s_arr[tid];
}
#+END_SRC
The solution: 
#+BEGIN_SRC c
__global__ void shiftLeftkernel(float *d_in, float *d_out){
  extern __shared__ float s_arr[];
  s_arr[tid] = d_in[tid];    // These 
  __syncthreads(); // Stop here
  float newval = s_arr[(tid + 1) % blockDim.x];
  __syncthreads(); // stop here
  s_arr[tid] = newval;
  // Now move data out to global memory
  d_out[tid] = s_arr[tid];
}
#+END_SRC
We have now put a barrier into the code, between the read and the write

** Fundamental algorithms in parallel
*** Reduce
- Input: List of elements, an associative binop on elements, and identity element
- Output: Scalar value of same type as element

One parallel red
#+BEGIN_SRC c++
__global__ void reduceParallel_sum(){
    for(int s=blockDim.x / 2; s > 0; s /=2){
        if (blockThreadId<s){
	    d_in[threadId] += d_in[threadId+s];
	}
	__syncthreads();
    }
}

#+END_SRC

Parallel reduce with shared mem
#+BEGIN_SRC c++
    extern __shared__ float s_data[];
    const unsigned int tid = threadIdx.x;


TODO

#+END_SRC

*** Exclusive scan
- Input: List of elements, bin assoc op, neut el
- List where each element is the reduction of all previous elements

We implement it in two sweeps. Blelloch scan

*** A note on shared mem
/When using shared mem we have about 48k of shared mem./  
/It is as fast as a register, but very limited in size/

To perform the exclusive scan across each block we perform excls scan on every block
then we get a list of reductions over each block(get the last element) and map that
to every scan
