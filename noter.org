
* Programming massively parralel hardware

* 01/09 Intro lecture
  SCHEDULED: <2020-09-01 Tue>
** Course organization
The couse has a Software track and a Hardware track
Intended learning outcomes of this lecture
- List Homorphisms
- Moores law and hardware trends
- Explain what a list-homomorphic program is /and/
- Be able to apply it to build programs
- Illustrate and apply the 1st Theorem of List HomoMorphism to transform said programs into
  inherently parralel ones

*** Lectures
- Tue 10:15 - 12:00, on zoom
- Thu 10:15 - 12:00, on zoom

*** Labs
- Thu: 13-17, 3-0-25, DIKU
- Half a class physically
- Other half on zoom
- /Will/ Accept students from virtual half as long as we do not overtake corona(20)
- If possible, may stay longer
*PURPOSE*
- Not educative(but you can ask questions)
- Use weekly ass to learn practical stuff
- Can't complete them without physical help
- *very important to attend the labs*

*** Assignments
Anders will provide feedback
- Four individual weekly assignments: 40% of grade
- One group project + final presentation and discussion: 60%
  - May choose from possible projects
  - Presented during lab at some point
  - Or discuss our own project
  - Can be very practical in CUDA, Futhark or more theoretical


** Brief history
*** Trend towards ever-increasing hardware parralellism(HP)
- Moores law(1960s)
Yeah u got this one
Generally aknowledged to be slowed down at this point

- ICPP, ISCA (1980/90s): parallel architechtures 
International conference for parallel processors

- Mid90 Killer-micro
Parallel was mostly seen as "Exotic"
The path of least resistance was to increase the speed of a Single CPU
On a single cpu the programmers point of view didn't change. Therefore
there was a lower barrier of entry, and easier maintenance of legacy software
Increasing single process performance "benefits everyone" with no change
in source
Commercially multiprocessors was just an uniprocessor extension

- Problems
Problem was. Power usage is cubed of the frequency, approx $P_dynamic \approx freq^3$
The Memory Wall. Performance gap between proc and mem increases

In 2004 Intel shifted focus to multi-cores instead

*** Biggest challenge for PH
A duality between the hardware system, and the application.

- Applications have performance requirements. 
- Hardware systems grant Performance opportunities
  
Thus application pressures the hardware to perform, and hardware
pressures the application to exploit opportunities.

The example is Microsoft and Intel, which Cosmin thinks about all the time
/at night/

The biggest challenge is to develop efficient Massively-Parralel Software
Think about programs with parallelism in mind rather than hack /some/ 
parallelism out of a sequential implementation(se node)

We need to think about developing for an infinite amount of course, since
we don't know how many cores will be available

** PMPH Main topics
*Hardware track* is the physical components of hardware design
- Processor (ILP, Intra and Inter-core)
- Memory hierarchy (coherency)
  - Several copies in the cache of the same mem-block
- Interconnect (inter-cores or core-cache routing)

*The Software track* studies programming models for data parallelism and
a way to reason and optimize parallelism
- High-level of abstraction
  - List-homomorphism $\equiv$ functional map-reduce style and flattening
- Low-level:
  - Loops and transformations rooted in data dependency analysis
  - c-programming

    
*The Lab* track applies in practice the optimizations/transformations learned
in the software track

The lecture notes will explain things to some extent, hardware book isn't mandatory

The hardware track is /important/ BUT the focus will be on software

** Abstractions in this course
A /program/ is to a /process or thread/ what a recipe is for cooking
/A processor(core)/ is a hardware entity capable of sequencing &
executing insts
/MT Cores/ Multiple threads in own context

** Software(List homomorphism)
Consider the realm of finite lists
.. denites list concatenation
Empty list is the neutral element eg [] (..) x === x (..) [] === x

LH is a special form of divide and conquer
we denote
h [] = e
h [x] = f x
h (x .. y) = (h x) (merged with a binop) (h y)

A well-defined program requires that no matter
how the input list is partitioned into, x .. y 
the result is the same

(compute the length of the list)
#+BEGIN_SRC 
len :: [T] -> Int
len [] = 0
len [x] = one(x) //where one(n) = 1 aka const
len (x..y) = (len x) + (len y)
#+END_SRC
That is the basic idea

What about a function that computes whether a list satisfies predicat p

#+BEGIN_SRC 
all_p :: [T] -> Bool
all_p [] = True // Because later we will probably use /and/ to compare
all_p [x] = p (x)
all_p (x..y) = (all_p x) && (all_p y)
#+END_SRC
(all_p x) is whether all elements of p returns true

*** Definition of monoid
Assume set $S$ and $binop : S x S \rightarrow S$. $(S,binop)$ is called a Monoid
If it satisfies the two axioms
1. Associativity $\forall x,y,z \in S$ we have $(x\ binop\ y)\ binop\ z \equiv x\ binop\ (y\ op\ z)$
2. Identity element: There exists $e \in S$ st. $\forall a \in S, e\ op\ a\ \equiv a\ op\ r\ \equiv a$

*** Definition of Monoid Homomorphism
A monoid homomorphism from monoid (S,op) to monoid (T,op2) is a function
h: S -> T st. $\forall u,v\in S, h(u op v) \equiv h(u) op2 h(v)


** TODO Basic blocks: Map
Map :: (al -> be) -> [al] -> [be] has inherently parralel semantics
| x = map f | [ a1, a2, a3 ]      |
| x'        | [f a1, f a2, f a3 ] |

** TODO Basic blocks Reduce
reduce applies an operator across all list elements

** 1.st lh theorem[Meertens]
#+BEGIN_SRC
h [] = e
h [x] = f x
h (x .. y) = (h x) (merged with a binop) (h y)
h === (reduce binop e) applied to (map f)
#+END_SRC

So in previous example
#+BEGIN_SRC 
len :: [T] -> Int
len [] = 0
len [x] = one(x) //where one(n) = 1 aka const
len (x..y) = (len x) + (len y)

len === (reduce (+) 0) on (map one)
#+END_SRC

And all_p
#+BEGIN_SRC 
all_p :: [T] -> Bool
all_p [] = True // Because later we will probably use /and/ to compare
all_p [x] = p (x)
all_p (x..y) = (all_p x) && (all_p y)

(reduce (&) True) on (map p)
#+END_SRC

I caml
#+BEGIN_SRC ocaml
let tmp = map p arr
let result = reduce (&) True tmp
#+END_SRC


** Theorem List-Homomorphism Promotions
Given unary functions f,g and an associative binop then
1. Map f on map g == map(f o g)
2. Mapping a (map f) on a list of lists, is equiv [map f [a1,...ai], map f [aj,...an]
   If we're reducing this with concatenation we get [f a1, f a2, ..., f an]
3. (reduce /binop/ e_binop) . (reduce (..) []) is equiv to
   (reduce /binop/ e_binop) . (map (reduce /binop/ e_binop)

** Distr
Assume =distr_p= distributes a list into /p/ sublist of roughly the same number of elements
Generally /p/ is the amount of processors

This is useful for code gen
(map f) . (reduce (..) [] = (reduce (..) []) . (map (map f)) 
map f = (reduce (..) []) . (map (map f)) . distr_p

In this case we're distributing the original list into a list of sublists, eg:  
[1,2,3,4] -> [[1,2], [3,4] ]
The reduce will be sequential, but the initial maps will be parralelised

** Theorem: Optimized map reduce
Assume =distrp= :: [a] ->  [[al] ] distributes a list. Denoting
=redomap binop f e_binop is.equiv (reduce binop e) . (map f)= The equality holds:  
redomap binop f e is.equiv (reduce /binop/ e) . (map (redomap /binop/ f e)) . distr_p
(nb: not redo map, but reduce o map)
Essentially, since the maps are run in parralel, we can reduce the maps first, then
when we finally flatten, we reduce only the already reduced list


** Maximum Segment Sum Problem
A non-homomorphjic function g can sometimes be lifted to a homomorphic one /f/
by computing a baggage of extra info

The initial function can be obtained by projecting the homomorphic result: 
/g = \pi . f/

MSSP:  
Given a list of integers, find the contiguous segment of the list whose members have 
the largest sum among all such segments. The result is only the sum

The naive attempt states:(where T denotes max)
#+BEGIN_SRC
mss [] = 0
mss [a] = a T 0
mss (x .. y) = mss(x) T? mss(y)
#+END_SRC
We get the issue that there may exist a larger mss that spans between the boundary between x and y

Therefore we compute extra information this time:
- Maximum concluding segment
  Always starts at the end, and whose sum is the maximum across such segments
- Maximum initial segment
  Always starts at the start, and whose sum is the max of such segments(inv. mcs)
- Total segment sum
  
At this point i introduce an example:
[1, -2, 3, 4, || -1, 5, -6, 1]
first list: 
- MSS = 7(3,4)
- MCS = 7(3,4)
- MIS = 6(1..4)
- TS  = 6
Second:
- MSS = 5
- MCS = 1(1)
- MIS = 4(-1,5)
- TS = -1

We now want to denote the functions =mis=,  
- /mis/ = max(mis_1, ts_1 + mis_2)  
  If the first initial is lower than the whole first segment + the second initial, then it spans 
  the whole first sublist and the initial of the second  
- /mcs/ = max(mcs_2, ts_2 + mcs_1)
  Same as before, but the other way aorund
- /mss/ = max(mss_1, mss_2, mcs_1 + mis_2)
  Either the mss of either split OR the split that crosses the boundary

#+BEGIN_SRC haskell
-- x T y = if(x >= y) then x else y
(mssx, misx, mcsx, tsx) O (mssy, misy, mcsy, tsy) = (
  mssx T mssy T (mcsx + misy),
  misx T (tsx+misy),
  (mcsx + tsy) T mcsy,
  tsx + tsy
)
f x = (x T 0, x T 0, x T 0)

emms = (reduce O (0,0,0,0)) . (map f)

mss = pi_1 . emss
# Where pi_1 (a,_,_,_) = a
#+END_SRC
The baggage is 3 extra integers (misx, mcsxm, tsx) and a constant number
of intops per communication stage
** Longest Satisfying Segment Problems
- A class of problems which requires to find the longest segment of a list
  for which some property holds such as
the longest sequence of zeros  
the longest sorted sequence

We restrict the shape of the predicate to:
#+BEGIN_SRC
p [] = True
P [x] = ...
p [x,y] = ...
p [ x : y : sz] = (p [x,y]) and p (y : zs)
#+END_SRC
We get extra baggage:
- As before, length of the longest initial/concluding satisfying segments (lis/lcs), and the 
  total list length (tl) 
- When considering the concat of (lcs,lis) pair, it is not guaranteed that the result 
  satisfies the predicate. E.G (sorted x) And (sorted y) != sorted x..y  
[1,2,3|2,3,4] both are sorted, but the concatenation isn't

therefore we need the last element of lcs and the first elem of lis

I.e. p [lastx,firsty] er sand så er de forbundet
sorted [3,2] == false så den forbinder ikke


* 03/09  Flattening
  SCHEDULED: <2020-09-03 Thu>

** Amdahl's law
Enhancement accelerates a fraction F of the task by a factos S
1. Speedup is limited by 1 - F that cannot be optimised
2. Optimize the common case & execute the rare case in software
3. Law of diminishing returns

In the context of Parallel Speedup
If we leave something sequential, it will become a bottleneck
as the amount of processors increases.

Typically speedup is sublinear, e.e. due to inter-thread communication

Even if 99% of the code is parralelized. We cannot move faster than the
1%

** PRAM Parallel Access Machine
Focuses exclusively on parallelism and ignores issues related to
synchro and comm:
1. p processors connected to shared memory
2. each processor has an unique id /i/
3. SIMD Execution, each parallel instruction requires unit time
4. Each processor has a flag that controls whether it is active in the execution of an instruction

The work time algorith WT
- Work Complexity W(n) is the total number of ops performed
- Depth/step complexity D(n) is the total number of sequential steps
If we know WT's work and depth, then Brents Theorem gives a good complexity bound
for a PRAM

** Brent Theorem
A work-Time algorithm of Depth D(n) and work W(n) can be simulated on a P-processor PRAM in time complex T  
$$ \frac{W(n)}{P} \leq T < \frac{W(n)}{P} + D(n) $$
